"""Code that evaluates the quality of the MVM products generated by the drizzlepac package.

The JSON files generated here can be converted directly into a Pandas DataFrame
using the syntax:

>>> import json
>>> import pandas as pd
>>> with open("<rootname>_astrometry_resids.json") as jfile:
>>>     resids = json.load(jfile)
>>> pdtab = pd.DataFrame(resids)

These DataFrames can then be concatenated using:

>>> allpd = pdtab.concat([pdtab2, pdtab3])

where 'pdtab2' and 'pdtab3' are DataFrames generated from other datasets.  For
more information on how to merge DataFrames, see

https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html

Visualization of these Pandas DataFrames with Bokeh can follow the example
from:

https://programminghistorian.org/en/lessons/visualizing-with-bokeh

"""

# Standard library imports
import argparse
from datetime import datetime
import os
import pickle
import sys
import time

import pdb

# Related third party imports
from astropy.io import fits

# Local application imports
from drizzlepac import util, wcs_functions
import drizzlepac.haputils.diagnostic_utils as du
import numpy as np
from drizzlepac.haputils import processing_utils as pu
from stsci.tools import logutil
from stwcs.wcsutil import HSTWCS

from drizzlepac.haputils.pandas_utils import PandasDFReader


__taskname__ = 'mvm_quality_analysis'

MSG_DATEFMT = '%Y%j%H%M%S'
SPLUNK_MSG_FORMAT = '%(asctime)s %(levelname)s src=%(name)s- %(message)s'
log = logutil.create_logger(__name__, level=logutil.logging.NOTSET, stream=sys.stdout,
                            format=SPLUNK_MSG_FORMAT, datefmt=MSG_DATEFMT)
# ----------------------------------------------------------------------------------------------------------------------

def report_wcsname(total_product_list, json_timestamp=None, json_time_since_epoch=None,
               log_level=logutil.logging.NOTSET):
    """Report the WCSNAME for each input exposure of an MVM product

    Parameters
    ----------
    total_product_list: list of HAP TotalProduct objects, one object per instrument detector
    (drizzlepac.haputils.Product.TotalProduct)

    json_timestamp: str, optional
        Universal .json file generation date and time (local timezone) that will be used in the instantiation
        of the HapDiagnostic object. Format: MM/DD/YYYYTHH:MM:SS (Example: 05/04/2020T13:46:35). If not
        specified, default value is logical 'None'

    json_time_since_epoch : float
        Universal .json file generation time that will be used in the instantiation of the HapDiagnostic
        object. Format: Time (in seconds) elapsed since January 1, 1970, 00:00:00 (UTC). If not specified,
        default value is logical 'None'

    log_level : int, optional
        The desired level of verboseness in the log statements displayed on the screen and
        written to the .log file.  Default value is 'NOTSET'.
    """
    log.setLevel(log_level)
    log.info('\n\n*****     Begin Quality Analysis Test: report_wcsname.     *****\n')
    aposteriori_list = ['FIT', 'SVM', 'MVM']

    # Generate a separate JSON file for each TotalProduct which is really a filter-level product for MVM processing
    # The "total product" references are a throw-back to SVM processing
    for total_product in total_product_list:
        detector = total_product.detector

        # Construct the output JSON filename
        json_filename = '_'.join([total_product.product_basename, 'mvm_wcsname.json'])

        # Set up the diagnostic object
        diagnostic_obj = du.HapDiagnostic()
        diagnostic_obj.instantiate_from_hap_obj(total_product,
                                                data_source='{}.report_wcsname'.format(__taskname__),
                                                description='WCS information',
                                                timestamp=json_timestamp,
                                                time_since_epoch=json_time_since_epoch)

        # Get the WCS for the entire MVM layer
        metawcs = HSTWCS(total_product.drizzle_filename, ext=1)

        # Loop over all the individual exposures in the list which comprise the layer
        counter = 0
        for edp_object in total_product.edp_list:

            # Get the WCSNAME for this specific exposure
            wcsname = fits.getval(edp_object.full_filename, 'WCSNAME', ext=1)

            # Obtain the value of S_REGION from all SCI extensions in the file
            # sregion is a list containing  numpy arrays
            sregion = pu.interpret_sregion(edp_object.full_filename, extname='SCI')

            all_sregion = np.concatenate(sregion, axis=0)

            ra = all_sregion[:,0].tolist()
            dec = all_sregion[:,1].tolist()

            # Convert RA, Dec to X, Y
            xpos, ypos = metawcs.all_world2pix(ra, dec, 1)
            x = xpos.tolist()
            y = ypos.tolist()
            
            # Load the dictionary with the collected data for this exposure
            active_wcs_dict = {'filename': edp_object.full_filename,
                               'primary_wcsname': wcsname,
                               'RA': ra,
                               'Dec': dec,
                               'X': x,
                               'Y': y}

            diagnostic_obj.add_data_item(active_wcs_dict, 'PrimaryWCS' + str(counter),
                                         descriptions={'filename': 'Exposure filename',
                                                       'primary_wcsname': 'Active WCS',
                                                       'RA': 'Right Ascension of S_REGION Polygon ICRS which defines footprint outline',
                                                       'Dec': 'Declination of S_REGION Polygon ICRS which defines footprint outline',
                                                       'X': 'X position of S_REGION Polygon which defines footprint outline',
                                                       'Y': 'Y position of S_REGION Polygon which defines footprint outline'},
                                         units={'filename': 'unitless',
                                                'primary_wcsname': 'unitless',
                                                'RA': 'degrees',
                                                'Dec': 'degrees',
                                                'X': 'pixels',
                                                'Y': 'pixels'})
            counter += 1

        # Write out the file
        diagnostic_obj.write_json_file(json_filename)

        # Clean up
        del diagnostic_obj

# ----------------------------------------------------------------------------------------------------------------------
def run_quality_analysis(total_obj_list,
                         run_report_wcsname=True,
                         log_level=logutil.logging.NOTSET):
    """Run the quality analysis functions

    Parameters
    ----------
    total_obj_list : list
        List of SkyCellProducts (equivalent of SVM FilterDataProducts)

    run_report_wcsname : bool, optional
        Run 'report_wcsname' test? Devault value is True.

    log_level : int, optional
        The desired level of verboseness in the log statements displayed on the screen and written to the
        .log file. Default value is 'NOTSET'.

    Returns
    -------
    Nothing.
    """
    log.setLevel(log_level)

    # generate a timestamp values that will be used to make creation time, creation date and epoch values
    # common to each json file
    json_timestamp = datetime.now().strftime("%m/%d/%YT%H:%M:%S")
    json_time_since_epoch = time.time()

    # Report WCSNAME
    if run_report_wcsname:
        try:
            report_wcsname(total_obj_list, json_timestamp=json_timestamp, json_time_since_epoch=json_time_since_epoch,
                       log_level=log_level)
        except Exception:
            log.warning("WCS reporting (report_wcsname) encountered a problem.")
            log.exception("message")
            log.warning("Continuing to next test...")


# ============================================================================================================


if __name__ == "__main__":
    # process command-line inputs with argparse
    parser = argparse.ArgumentParser(description='Perform quality assessments of the MVM products generated '
                                                 'by the drizzlepac package. NOTE: if no QA switches '
                                                 'are specified, ALL QA steps will be executed.')
    parser.add_argument('input_filename', help='_total_list.pickle file that holds vital information about '
                                               'the processing run')
    parser.add_argument('-wcs', '--run_report_wcsname', required=False, action='store_true',
                        help="Report the WCSNAME information for each exposure of an MVM layer product")
    parser.add_argument('-l', '--log_level', required=False, default='info',
                        choices=['critical', 'error', 'warning', 'info', 'debug'],
                        help='The desired level of verboseness in the log statements displayed on the screen '
                             'and written to the .log file. The level of verboseness from left to right, and '
                             'includes all log statements with a log_level left of the specified level. '
                             'Specifying "critical" will only record/display "critical" log statements, and '
                             'specifying "error" will record/display both "error" and "critical" log '
                             'statements, and so on.')
    user_args = parser.parse_args()

    # set up logging
    log_dict = {"critical": logutil.logging.CRITICAL,
                "error": logutil.logging.ERROR,
                "warning": logutil.logging.WARNING,
                "info": logutil.logging.INFO,
                "debug": logutil.logging.DEBUG}
    log_level = log_dict[user_args.log_level]
    log.setLevel(log_level)

    # verify that input file exists
    if not os.path.exists(user_args.input_filename):
        err_msg = "File {} doesn't exist.".format(user_args.input_filename)
        log.critical(err_msg)
        raise Exception(err_msg)

    #  check that at least one QA switch is turned on
    all_qa_steps_off = True
    max_step_str_length = 0
    for kv_pair in user_args._get_kwargs():
        if kv_pair[0] not in ['input_filename', 'run_all', 'log_level']:
            if len(kv_pair[0])-4 > max_step_str_length:
                max_step_str_length = len(kv_pair[0])-4
            if kv_pair[1]:
                all_qa_steps_off = False

    # if no QA steps are explicitly turned on in the command-line call, run ALL the QA steps
    if all_qa_steps_off:
        log.info("No specific QA switches were turned on. All QA steps will be executed.")
        user_args.run_report_wcs = True

    # display status summary indicating which QA steps are turned on and which steps are turned off
    toplinestring = "-"*(int(max_step_str_length/2)-6)
    log.info("{}QA step run status{}".format(toplinestring, toplinestring))
    for kv_pair in user_args._get_kwargs():
        if kv_pair[0] not in ['input_filename', 'run_all', 'log_level']:
            if kv_pair[1]:
                run_status = "ON"
            else:
                run_status = "off"
            log.info("{}{}   {}".format(kv_pair[0][4:], " "*(max_step_str_length-(len(kv_pair[0])-4)),
                                        run_status))
    log.info("-"*(max_step_str_length+6))

    # execute specified tests
    filehandler = open(user_args.input_filename, 'rb')
    total_obj_list = pickle.load(filehandler)
    run_quality_analysis(total_obj_list,
                         run_report_wcs=user_args.run_report_wcs,
                         log_level=log_level)

